{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from email.parser import Parser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'id_mail': [],\n",
    "        'date': [],\n",
    "        'from': [],\n",
    "        'to': [],\n",
    "        'subject': [],\n",
    "        'body': [],\n",
    "        'x_origin': [],\n",
    "        }\n",
    "for path in Path('data').rglob('*/all_documents/*'):\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as file:\n",
    "        # print(path, \"\\n\\n***\")\n",
    "        \n",
    "        raw_email = Parser().parse(file)\n",
    "        # print(path, \"\\n\\n***\")\n",
    "        # print(raw_email)\n",
    "        data[\"id_mail\"].append(raw_email.get('Message-ID'))\n",
    "        data[\"x_origin\"].append(raw_email.get('X-Origin'))\n",
    "        data[\"from\"].append(raw_email.get('From'))\n",
    "        data[\"to\"].append(raw_email.get('To'))\n",
    "        data[\"subject\"].append(raw_email.get('Subject'))\n",
    "        data[\"date\"].append(raw_email.get('Date'))\n",
    "        data[\"body\"].append(raw_email.get_payload())\n",
    "        # print(\"*\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_mail</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>x_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;28040030.1075840228655.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Fri, 8 Dec 2000 07:49:00 -0800 (PST)</td>\n",
       "      <td>ralph.blakemore@enron.com</td>\n",
       "      <td>kenneth.lay@enron.com, jeff.skilling@enron.com</td>\n",
       "      <td>2000 Chairman's Award</td>\n",
       "      <td>Gentlemen,\\n\\nThank you for the letter memoria...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;30356390.1075840228680.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 11 Dec 2000 01:52:00 -0800 (PST)</td>\n",
       "      <td>barbara.paige@enron.com</td>\n",
       "      <td>samantha.bryce@enron.com, adriana.cortes@enron...</td>\n",
       "      <td>Field Study Program</td>\n",
       "      <td>A description of the proposed Field Study Prog...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;4711296.1075840228704.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 11 Dec 2000 09:02:00 -0800 (PST)</td>\n",
       "      <td>william.ramsay@iea.org</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>RE: IEA Ministerial/OECD Ministerial</td>\n",
       "      <td>Rosalee:  Thank you for this good news.  I wil...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;27851547.1075840228727.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 11 Dec 2000 02:49:00 -0800 (PST)</td>\n",
       "      <td>rosalee.fleming@enron.com</td>\n",
       "      <td>shea_dugger@i2.com</td>\n",
       "      <td>Re: Final Eagle BOD Presentation</td>\n",
       "      <td>Here it is!!  Have a great day!!  Stay warm.\\n...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;16638165.1075840228750.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 11 Dec 2000 04:01:00 -0800 (PST)</td>\n",
       "      <td>nancy@newcapitolsolutions.com</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>Richard's Resume</td>\n",
       "      <td>Dear Ken,\\n\\nMy son, Richard, recently sold hi...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id_mail  \\\n",
       "0  <28040030.1075840228655.JavaMail.evans@thyme>   \n",
       "1  <30356390.1075840228680.JavaMail.evans@thyme>   \n",
       "2   <4711296.1075840228704.JavaMail.evans@thyme>   \n",
       "3  <27851547.1075840228727.JavaMail.evans@thyme>   \n",
       "4  <16638165.1075840228750.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    date                           from  \\\n",
       "0   Fri, 8 Dec 2000 07:49:00 -0800 (PST)      ralph.blakemore@enron.com   \n",
       "1  Mon, 11 Dec 2000 01:52:00 -0800 (PST)        barbara.paige@enron.com   \n",
       "2  Mon, 11 Dec 2000 09:02:00 -0800 (PST)         william.ramsay@iea.org   \n",
       "3  Mon, 11 Dec 2000 02:49:00 -0800 (PST)      rosalee.fleming@enron.com   \n",
       "4  Mon, 11 Dec 2000 04:01:00 -0800 (PST)  nancy@newcapitolsolutions.com   \n",
       "\n",
       "                                                  to  \\\n",
       "0     kenneth.lay@enron.com, jeff.skilling@enron.com   \n",
       "1  samantha.bryce@enron.com, adriana.cortes@enron...   \n",
       "2                              kenneth.lay@enron.com   \n",
       "3                                 shea_dugger@i2.com   \n",
       "4                              kenneth.lay@enron.com   \n",
       "\n",
       "                                subject  \\\n",
       "0                 2000 Chairman's Award   \n",
       "1                   Field Study Program   \n",
       "2  RE: IEA Ministerial/OECD Ministerial   \n",
       "3      Re: Final Eagle BOD Presentation   \n",
       "4                      Richard's Resume   \n",
       "\n",
       "                                                body x_origin  \n",
       "0  Gentlemen,\\n\\nThank you for the letter memoria...    LAY-K  \n",
       "1  A description of the proposed Field Study Prog...    LAY-K  \n",
       "2  Rosalee:  Thank you for this good news.  I wil...    LAY-K  \n",
       "3  Here it is!!  Have a great day!!  Stay warm.\\n...    LAY-K  \n",
       "4  Dear Ken,\\n\\nMy son, Richard, recently sold hi...    LAY-K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1961, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_mail     object\n",
       "date        object\n",
       "from        object\n",
       "to          object\n",
       "subject     object\n",
       "body        object\n",
       "x_origin    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentlemen,\n",
      "\n",
      "Thank you for the letter memorializing my nomination for the 2000 Chairman's \n",
      "Award.  It is a privilege to be a part of the Enron organization.  I have \n",
      "received many promotions, cash bonuses and commendations during my career but \n",
      "I consider the recognition as a nominee for the 2000 Chairman's Award to be \n",
      "the most significant acknowledgment of my performance ever received.  My \n",
      "compliments to you for making Enron one of the most successful and excellent \n",
      "companies on the globe.  I only wish that I could contribute more to Enron's \n",
      "growth and continued success.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "RW Blakemore\n",
      "Enron Wind Energy Systems Corp.\n",
      "Center For Advanced Technology \n"
     ]
    }
   ],
   "source": [
    "# a sample email\n",
    "print(df.loc[0]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri, 8 Dec 2000 07:49:00 -0800 (PST)\n"
     ]
    }
   ],
   "source": [
    "# get the date\n",
    "print(df.loc[0]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    LAY-K\n",
       "1    LAY-K\n",
       "2    LAY-K\n",
       "3    LAY-K\n",
       "4    LAY-K\n",
       "5    LAY-K\n",
       "6    LAY-K\n",
       "7    LAY-K\n",
       "8    LAY-K\n",
       "9    LAY-K\n",
       "Name: x_origin, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Employ names\n",
    "df['x_origin'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_origin</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAY-K</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SKILLING-J</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_origin  Counts\n",
       "0       LAY-K    1127\n",
       "1  SKILLING-J     834"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Higest email sender\n",
    "top = pd.DataFrame(df['x_origin'].value_counts()[:2])\n",
    "top.reset_index(inplace=True)\n",
    "top.columns = [\"x_origin\", \"Counts\"]\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-05-2001 13:51:00\n"
     ]
    }
   ],
   "source": [
    "# cleaning data column\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# this is sample example\n",
    "x = parser.parse(\"Fri, 4 May 2001 13:51:00 -0700 (PDT)\")\n",
    "print(x.strftime(\"%d-%m-%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -date format tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_mail</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>x_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;28040030.1075840228655.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>08-12-2000 07:49:00</td>\n",
       "      <td>ralph.blakemore@enron.com</td>\n",
       "      <td>kenneth.lay@enron.com, jeff.skilling@enron.com</td>\n",
       "      <td>2000 Chairman's Award</td>\n",
       "      <td>Gentlemen,\\n\\nThank you for the letter memoria...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;30356390.1075840228680.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>11-12-2000 01:52:00</td>\n",
       "      <td>barbara.paige@enron.com</td>\n",
       "      <td>samantha.bryce@enron.com, adriana.cortes@enron...</td>\n",
       "      <td>Field Study Program</td>\n",
       "      <td>A description of the proposed Field Study Prog...</td>\n",
       "      <td>LAY-K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id_mail                 date  \\\n",
       "0  <28040030.1075840228655.JavaMail.evans@thyme>  08-12-2000 07:49:00   \n",
       "1  <30356390.1075840228680.JavaMail.evans@thyme>  11-12-2000 01:52:00   \n",
       "\n",
       "                        from  \\\n",
       "0  ralph.blakemore@enron.com   \n",
       "1    barbara.paige@enron.com   \n",
       "\n",
       "                                                  to                subject  \\\n",
       "0     kenneth.lay@enron.com, jeff.skilling@enron.com  2000 Chairman's Award   \n",
       "1  samantha.bryce@enron.com, adriana.cortes@enron...    Field Study Program   \n",
       "\n",
       "                                                body x_origin  \n",
       "0  Gentlemen,\\n\\nThank you for the letter memoria...    LAY-K  \n",
       "1  A description of the proposed Field Study Prog...    LAY-K  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_type(dates):\n",
    "    column = []\n",
    "    \n",
    "    for date in dates:\n",
    "        column.append(parser.parse(date).strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "    return column\n",
    "\n",
    "df['date'] = change_type(df['date'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_mail      0\n",
       "date         0\n",
       "from         0\n",
       "to          81\n",
       "subject      0\n",
       "body         0\n",
       "x_origin     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sayra\\Desktop\\Enron_Email_Usecase\\main.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sayra/Desktop/Enron_Email_Usecase/main.ipynb#ch0000020?line=5'>6</a>\u001b[0m words \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sayra/Desktop/Enron_Email_Usecase/main.ipynb#ch0000020?line=7'>8</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sayra/Desktop/Enron_Email_Usecase/main.ipynb#ch0000020?line=8'>9</a>\u001b[0m word_tokens \u001b[39m=\u001b[39m word_tokenize(words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sayra/Desktop/Enron_Email_Usecase/main.ipynb#ch0000020?line=9'>10</a>\u001b[0m useful_words \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words_tokens \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sayra/Desktop/Enron_Email_Usecase/main.ipynb#ch0000020?line=11'>12</a>\u001b[0m frequency \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mFreqDist(useful_words)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=114'>115</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=115'>116</a>\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=116'>117</a>\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=126'>127</a>\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=127'>128</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=128'>129</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=129'>130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=130'>131</a>\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=131'>132</a>\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=96'>97</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=97'>98</a>\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=98'>99</a>\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=103'>104</a>\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=104'>105</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=105'>106</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/__init__.py?line=106'>107</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1271'>1272</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1272'>1273</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1273'>1274</a>\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1274'>1275</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1275'>1276</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1324'>1325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1325'>1326</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1326'>1327</a>\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1327'>1328</a>\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1328'>1329</a>\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1329'>1330</a>\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1330'>1331</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1331'>1332</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1324'>1325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1325'>1326</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1326'>1327</a>\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1327'>1328</a>\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1328'>1329</a>\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1329'>1330</a>\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1330'>1331</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1331'>1332</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1319'>1320</a>\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1320'>1321</a>\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1321'>1322</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1322'>1323</a>\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1407'>1408</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1408'>1409</a>\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1409'>1410</a>\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1417'>1418</a>\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1418'>1419</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1419'>1420</a>\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1420'>1421</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1421'>1422</a>\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1422'>1423</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=315'>316</a>\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=316'>317</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=317'>318</a>\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=318'>319</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=319'>320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1392'>1393</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1393'>1394</a>\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1394'>1395</a>\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1395'>1396</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1396'>1397</a>\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1372'>1373</a>\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1373'>1374</a>\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[1;32m-> <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1374'>1375</a>\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1375'>1376</a>\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1376'>1377</a>\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[0;32m   <a href='file:///c%3A/Users/sayra/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/tokenize/punkt.py?line=1377'>1378</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Most common words in the email\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words = df['body']\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(words)\n",
    "useful_words = [w for w in words_tokens if w not in stop_words]\n",
    "\n",
    "frequency = nltk.FreqDist(useful_words)\n",
    "\n",
    "# print(frequency.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords  sample\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(stopwords.words('english'))\n",
    " \n",
    "# example_sent = \"\"\"This is a sample sentence,\n",
    "#                   showing off the stop words filtration.\"\"\"\n",
    " \n",
    "# stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "# filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "# filtered_sentence = []\n",
    " \n",
    "# for w in word_tokens:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    " \n",
    "# print(word_tokens)\n",
    "# print(filtered_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Gentlemen,\\n\\nThank you for the letter memoria...\n",
      "1       A description of the proposed Field Study Prog...\n",
      "2       Rosalee:  Thank you for this good news.  I wil...\n",
      "3       Here it is!!  Have a great day!!  Stay warm.\\n...\n",
      "4       Dear Ken,\\n\\nMy son, Richard, recently sold hi...\n",
      "                              ...                        \n",
      "1956    Dear Mr. Paul,\\n\\nI'm writing on behalf of Jef...\n",
      "1957    It is becoming increasingly clear that the dev...\n",
      "1958    Ann, I haven't heard from you with regard to w...\n",
      "1959    I shall be there.  Just let me know what time ...\n",
      "1960    Mom and Dad,\\n\\nGot mom's letter last week; an...\n",
      "Name: body, Length: 1961, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc3f249eb8269fe0400e72a0f427e175b24dd81fbe32ee1422536ef70707752a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
