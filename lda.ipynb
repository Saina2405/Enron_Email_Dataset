{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "sns.set() \n",
    "\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from dateutil import parser\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Display setting to show more characters in column\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come', 'com', 'http', 'mail', 'pm'])\n",
    "\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data/raw_mail_all.csv\").sample(5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"], infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(\"\\S*@\\S*\\s?\", \"\", sent)  # remove emails\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)  # remove newline chars\n",
    "        sent = re.sub(\"'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield (sent)\n",
    "\n",
    "\n",
    "data = df.body.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(\n",
    "    texts, stop_words=stop_words, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\"]\n",
    "):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts_out\n",
    "    ]\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "data_ready = process_words(data_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_ready[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_ready, open( \"clean_words.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = pickle.load( open( \"clean_words.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=5,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=60,\n",
    "    alpha=\"auto\",\n",
    "    iterations=100,\n",
    "    per_word_topics=True,\n",
    ")\n",
    "\n",
    "print(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = [\"Dominant_Topic\", \"Perc_Contribution\", \"Topic_Keywords\"]\n",
    "\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=lda_model, corpus=corpus, texts=data_ready\n",
    ")\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    \"Document_No\",\n",
    "    \"Dominant_Topic\",\n",
    "    \"Topic_Perc_Contrib\",\n",
    "    \"Keywords\",\n",
    "    \"Text\",\n",
    "]\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby(\"Dominant_Topic\")\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat(\n",
    "        [\n",
    "            sent_topics_sorteddf_mallet,\n",
    "            grp.sort_values([\"Perc_Contribution\"], ascending=False).head(1),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "# Reset Index\n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = [\n",
    "    \"Topic_Num\",\n",
    "    \"Topic_Perc_Contrib\",\n",
    "    \"Keywords\",\n",
    "    \"Representative Text\",\n",
    "]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(f\"test\"))\n",
    "\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(f\"test\") +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top modeling visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 7), dpi=160)\n",
    "plt.hist(doc_lens, bins=1000, color=\"navy\")\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750, 90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750, 80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750, 70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750, 60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(\n",
    "    xlim=(0, 1000), ylabel=\"Number of Documents\", xlabel=\"Document Word Count\"\n",
    ")\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0, 1000, 9))\n",
    "plt.title(\"Distribution of Document Word Counts\", fontdict=dict(size=22))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [\n",
    "    color for name, color in mcolors.TABLEAU_COLORS.items()\n",
    "]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[\n",
    "        df_dominant_topic.Dominant_Topic == i, :\n",
    "    ]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins=1000, color=cols[i])\n",
    "    ax.tick_params(axis=\"y\", labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel=\"Document Word Count\")\n",
    "    ax.set_ylabel(\"Number of Documents\", color=cols[i])\n",
    "    ax.set_title(\"Topic: \" + str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0, 1000, 9))\n",
    "fig.suptitle(\"Distribution of Document Word Counts by Dominant Topic\", fontsize=22)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [\n",
    "    color for name, color in mcolors.TABLEAU_COLORS.items()\n",
    "]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(\n",
    "    stopwords=stop_words,\n",
    "    background_color=\"white\",\n",
    "    width=2500,\n",
    "    height=1800,\n",
    "    max_words=10,\n",
    "    colormap=\"tab10\",\n",
    "    color_func=lambda *args, **kwargs: cols[i],\n",
    "    prefer_horizontal=1.0,\n",
    ")\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title(\"Topic \" + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis(\"off\")\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i, weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"word\", \"topic_id\", \"importance\", \"word_count\"])\n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(\n",
    "        x=\"word\",\n",
    "        height=\"word_count\",\n",
    "        data=df.loc[df.topic_id == i, :],\n",
    "        color=cols[i],\n",
    "        width=0.5,\n",
    "        alpha=0.3,\n",
    "        label=\"Word Count\",\n",
    "    )\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(\n",
    "        x=\"word\",\n",
    "        height=\"importance\",\n",
    "        data=df.loc[df.topic_id == i, :],\n",
    "        color=cols[i],\n",
    "        width=0.2,\n",
    "        label=\"Weights\",\n",
    "    )\n",
    "    ax.set_ylabel(\"Word Count\", color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030)\n",
    "    ax.set_ylim(0, 3500)\n",
    "    ax.set_title(\"Topic: \" + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis=\"y\", left=False)\n",
    "    ax.set_xticklabels(\n",
    "        df.loc[df.topic_id == i, \"word\"], rotation=30, horizontalalignment=\"right\"\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax_twin.legend(loc=\"upper right\")\n",
    "\n",
    "fig.tight_layout(w_pad=2)\n",
    "fig.suptitle(\"Word Count and Importance of Topic Keywords\", fontsize=22, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start=0, end=13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        end - start, 1, figsize=(20, (end - start) * 0.95), dpi=160\n",
    "    )\n",
    "    axes[0].axis(\"off\")\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i - 1]\n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [\n",
    "                (lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics\n",
    "            ]\n",
    "            ax.text(\n",
    "                0.01,\n",
    "                0.5,\n",
    "                \"Doc \" + str(i - 1) + \": \",\n",
    "                verticalalignment=\"center\",\n",
    "                fontsize=16,\n",
    "                color=\"black\",\n",
    "                transform=ax.transAxes,\n",
    "                fontweight=700,\n",
    "            )\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(\n",
    "                Rectangle(\n",
    "                    (0.0, 0.05),\n",
    "                    0.99,\n",
    "                    0.90,\n",
    "                    fill=None,\n",
    "                    alpha=1,\n",
    "                    color=mycolors[topic_percs_sorted[0][0]],\n",
    "                    linewidth=2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(\n",
    "                        word_pos,\n",
    "                        0.5,\n",
    "                        word,\n",
    "                        horizontalalignment=\"left\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        fontsize=16,\n",
    "                        color=mycolors[topics],\n",
    "                        transform=ax.transAxes,\n",
    "                        fontweight=700,\n",
    "                    )\n",
    "                    word_pos += 0.009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis(\"off\")\n",
    "            ax.text(\n",
    "                word_pos,\n",
    "                0.5,\n",
    "                \". . .\",\n",
    "                horizontalalignment=\"left\",\n",
    "                verticalalignment=\"center\",\n",
    "                fontsize=16,\n",
    "                color=\"black\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle(\n",
    "        \"Sentence Topic Coloring for Documents: \" + str(start) + \" to \" + str(end - 2),\n",
    "        fontsize=22,\n",
    "        y=0.95,\n",
    "        fontweight=700,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sentences_chart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impliment K Means Cluster Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=300, n_init=1, verbose=1)\n",
    "model.fit(X_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(model, \"model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_.argsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Words Per Cluster\n",
    "\n",
    "https://github.com/adriancampos1/Enron_Email_Analysis/blob/master/Enron_Email_Analysis_K-means_clustering.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster d:\", i),\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(\" \", terms[ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Test Document & Find it's Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming we still have our 'body' document\n",
    "test_document = df[\"body\"][0]\n",
    "test_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = cv.transform([test_document])\n",
    "test_X_dense = test_X.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_X_df = pd.DataFrame(test_X_dense, columns=cv.get_feature_names())\n",
    "# creating new dense to correct for chopped of features earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_X_df = text_X_df.iloc[:, 106:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_X_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction\")\n",
    "predicted = model.predict(text_X_df)\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c=\"black\", s=200, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(X_df)\n",
    "\n",
    "centers = model.cluster_centers_\n",
    "pca_centers = pca.transform(centers)\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], c=\"m\")\n",
    "plt.scatter(pca_centers[:, 0], pca_centers[:, 1], c=\"red\", s=200, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(model.cluster_centers_)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc3f249eb8269fe0400e72a0f427e175b24dd81fbe32ee1422536ef70707752a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
