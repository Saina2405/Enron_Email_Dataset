{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend(\n",
    "    [\n",
    "        \"from\",\n",
    "        \"subject\",\n",
    "        \"re\",\n",
    "        \"edu\",\n",
    "        \"use\",\n",
    "        \"not\",\n",
    "        \"would\",\n",
    "        \"say\",\n",
    "        \"could\",\n",
    "        \"_\",\n",
    "        \"be\",\n",
    "        \"know\",\n",
    "        \"good\",\n",
    "        \"go\",\n",
    "        \"get\",\n",
    "        \"do\",\n",
    "        \"done\",\n",
    "        \"try\",\n",
    "        \"many\",\n",
    "        \"some\",\n",
    "        \"nice\",\n",
    "        \"thank\",\n",
    "        \"think\",\n",
    "        \"see\",\n",
    "        \"rather\",\n",
    "        \"easy\",\n",
    "        \"easily\",\n",
    "        \"lot\",\n",
    "        \"lack\",\n",
    "        \"make\",\n",
    "        \"want\",\n",
    "        \"seem\",\n",
    "        \"run\",\n",
    "        \"need\",\n",
    "        \"even\",\n",
    "        \"right\",\n",
    "        \"line\",\n",
    "        \"even\",\n",
    "        \"also\",\n",
    "        \"may\",\n",
    "        \"take\",\n",
    "        \"come\",\n",
    "        \"com\",\n",
    "        \"http\",\n",
    "        \"mail\",\n",
    "        \"pm\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"../data/raw_mail_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"], infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(\"\\S*@\\S*\\s?\", \"\", sent)  # remove emails\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)  # remove newline chars\n",
    "        sent = re.sub(\"'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield (sent)\n",
    "\n",
    "\n",
    "data = df.body.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pickle.dump(data, open(\"../data/data.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(\n",
    "    texts, stop_words=stop_words, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\"]\n",
    "):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts_out\n",
    "    ]\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "data_ready = process_words(data_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_ready, open(\"../data/clean_words.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc3f249eb8269fe0400e72a0f427e175b24dd81fbe32ee1422536ef70707752a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
