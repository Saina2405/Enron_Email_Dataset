{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8100d4bd",
   "metadata": {},
   "source": [
    "## Creation of a corpus based on the body of our CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60db9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159c6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data\\v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ed1328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128103, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab26d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128103 entries, 0 to 128102\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  128103 non-null  int64 \n",
      " 1   id_mail     128103 non-null  object\n",
      " 2   date        128103 non-null  object\n",
      " 3   from        128103 non-null  object\n",
      " 4   to          125357 non-null  object\n",
      " 5   subject     123580 non-null  object\n",
      " 6   body        128103 non-null  object\n",
      " 7   x_origin    128103 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 7.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd46626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ---------------------- Forwarded by Phillip K ...\n",
       "1    ---------------------- Forwarded by Phillip K ...\n",
       "2    Dave, \\n\\n Here are the names of the west desk...\n",
       "3             Paula,\\n\\n 35 million is fine\\n\\nPhillip\n",
       "4    ---------------------- Forwarded by Phillip K ...\n",
       "5    ---------------------- Forwarded by Phillip K ...\n",
       "6    ---------------------- Forwarded by Phillip K ...\n",
       "7    Brenda,\\n\\nPlease use the second check as the ...\n",
       "8    I think Fletch has a good CPA.  I am still doi...\n",
       "9    Brenda,\\n\\n Please use the second check as my ...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c5ea74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af41403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eeab68",
   "metadata": {},
   "source": [
    "### 1. First approach using regex and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb94ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    # porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    # stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e074621",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_cleaned = []\n",
    "for body in df[\"body\"]:\n",
    "    body_cleaned.append(clean(body).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "432653a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596f8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with tokenization\n",
    "body_token = []\n",
    "for body in df[\"body\"]:\n",
    "    body_token.append(clean(body).split())\n",
    "body_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a621d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove small words (len lower than 3)\n",
    "for list in body_cleaned:\n",
    "    for word in list:\n",
    "        if len(word) <= 3:\n",
    "            list.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b27ede05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_cleaned[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "65d41c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a03b28e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128103"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e3eac138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_clean'] = body_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fb9c7419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [forwarded, phillip, allen, richard, burchfiel...\n",
       "1    [forwarded, phillip, allen, richard, burchfiel...\n",
       "2    [dave, name, west, desk, member, category, ori...\n",
       "3                      [paula, million, fine, phillip]\n",
       "4    [forwarded, phillip, allen, enron, north, amer...\n",
       "5    [forwarded, phillip, allen, george, richards, ...\n",
       "6    [forwarded, phillip, allen, nancy, hall, enron...\n",
       "7    [brenda, please, second, check, october, payme...\n",
       "8                         [think, fletch, good, still]\n",
       "9    [brenda, please, second, check, october, payme...\n",
       "Name: body_clean, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body_clean'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3610b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('position', 33), ('deal', 11), ('project', 9), ('manager', 8), ('ability', 8), ('consolidated', 7), ('issue', 7), ('physical', 7), ('need', 7), ('requirement', 6), ('meeting', 6), ('trader', 6), ('option', 6), ('list', 5), ('screen', 5)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency for each email\n",
    "from nltk import FreqDist\n",
    "frequency = nltk.FreqDist(body_cleaned[0])\n",
    "print(frequency.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ddbac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new list\n",
    "most_common = []\n",
    "for item in body_cleaned:\n",
    "    frequency = nltk.FreqDist(item)\n",
    "    most_common.append(frequency.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b37bbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['most_common_15'] = most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c6c86d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da18bf",
   "metadata": {},
   "source": [
    "### 2. Second approach using word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "01477428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forwarded',\n",
       " 'phillip',\n",
       " 'allen hou ect',\n",
       " '          ',\n",
       " '     ',\n",
       " 'richard',\n",
       " 'burchfield',\n",
       " '          ',\n",
       " '     ',\n",
       " 'phillip',\n",
       " 'allen hou ect',\n",
       " 'beth',\n",
       " 'perlman hou ect',\n",
       " 'subject',\n",
       " 'consolidated',\n",
       " 'positions',\n",
       " 'issues',\n",
       " 'list',\n",
       " 'phillip',\n",
       " 'issues',\n",
       " 'list',\n",
       " 'forward',\n",
       " 'documenting',\n",
       " 'requirements',\n",
       " 'consolidated',\n",
       " 'physical financial',\n",
       " 'positions',\n",
       " 'transport',\n",
       " 'trade',\n",
       " 'capture',\n",
       " 'need',\n",
       " 'focus',\n",
       " 'first',\n",
       " 'bullet',\n",
       " 'allan',\n",
       " 'list',\n",
       " 'need',\n",
       " 'single',\n",
       " 'requirements',\n",
       " 'although',\n",
       " 'meeting',\n",
       " 'keith',\n",
       " 'wednesday',\n",
       " 'informative',\n",
       " 'solution',\n",
       " 'creating',\n",
       " 'infinitely',\n",
       " 'dynamic',\n",
       " 'consolidated',\n",
       " 'position',\n",
       " 'screen',\n",
       " 'extremely',\n",
       " 'difficult',\n",
       " 'time',\n",
       " 'consuming',\n",
       " 'throughout',\n",
       " 'meeting',\n",
       " 'wednesday',\n",
       " 'keith',\n",
       " 'alluded',\n",
       " 'inability',\n",
       " 'consensus',\n",
       " 'amongst',\n",
       " 'traders',\n",
       " 'presentation',\n",
       " 'consolidated',\n",
       " 'position',\n",
       " 'solution',\n",
       " 'make',\n",
       " 'trader',\n",
       " 'arrange',\n",
       " 'position',\n",
       " 'screen',\n",
       " 'liking',\n",
       " 'much',\n",
       " 'like',\n",
       " 'excel',\n",
       " 'needs',\n",
       " 'happen',\n",
       " 'monday',\n",
       " 'effort',\n",
       " 'design',\n",
       " 'desired',\n",
       " 'layout',\n",
       " 'consolidated',\n",
       " 'position',\n",
       " 'screen',\n",
       " 'critical',\n",
       " 'exclude',\n",
       " 'building',\n",
       " 'capability',\n",
       " 'create',\n",
       " 'flexible',\n",
       " 'position',\n",
       " 'presentation',\n",
       " 'future',\n",
       " 'order',\n",
       " 'create',\n",
       " 'plan',\n",
       " 'measured',\n",
       " 'need',\n",
       " 'firm',\n",
       " 'requirements',\n",
       " 'also',\n",
       " 'reiterate',\n",
       " 'goals',\n",
       " 'project',\n",
       " 'project',\n",
       " 'plan',\n",
       " 'consolidate',\n",
       " 'physical financial',\n",
       " 'positions',\n",
       " 'transport',\n",
       " 'trade',\n",
       " 'capture',\n",
       " 'issues',\n",
       " 'raised',\n",
       " 'capture',\n",
       " 'projects',\n",
       " 'need',\n",
       " 'prioritised',\n",
       " 'efforts',\n",
       " 'outside',\n",
       " 'project',\n",
       " 'involved',\n",
       " 'meetings',\n",
       " 'discussions',\n",
       " 'good',\n",
       " 'believe',\n",
       " 'good',\n",
       " 'communication',\n",
       " 'teams',\n",
       " 'need',\n",
       " 'focus',\n",
       " 'objectives',\n",
       " 'solve',\n",
       " 'richard',\n",
       " 'forwarded',\n",
       " 'richard',\n",
       " 'burchfield hou ect',\n",
       " '          ',\n",
       " '     ',\n",
       " 'allan',\n",
       " 'severude',\n",
       " '          ',\n",
       " '     ',\n",
       " 'richard',\n",
       " 'burchfield hou ect',\n",
       " 'peggy',\n",
       " 'alix hou ect',\n",
       " 'russ',\n",
       " 'severson hou ect',\n",
       " 'scott',\n",
       " 'mills hou ect',\n",
       " 'kenny',\n",
       " 'ha hou ect',\n",
       " 'subject',\n",
       " 'consolidated',\n",
       " 'positions',\n",
       " 'issues',\n",
       " 'list',\n",
       " 'initial',\n",
       " 'meetings',\n",
       " 'traders',\n",
       " 'regarding',\n",
       " 'consolidated',\n",
       " 'positions',\n",
       " 'think',\n",
       " 'still',\n",
       " 'following',\n",
       " 'issues',\n",
       " 'single',\n",
       " 'point',\n",
       " 'contact',\n",
       " 'trading',\n",
       " 'group',\n",
       " 'three',\n",
       " 'meetings',\n",
       " 'brought',\n",
       " 'different',\n",
       " 'issues',\n",
       " 'different',\n",
       " 'traders',\n",
       " 'really',\n",
       " 'need',\n",
       " 'single',\n",
       " 'point',\n",
       " 'contact',\n",
       " 'help',\n",
       " 'drive',\n",
       " 'trader',\n",
       " 'requirements',\n",
       " 'help',\n",
       " 'come',\n",
       " 'consensus',\n",
       " 'regarding',\n",
       " 'requirements',\n",
       " 'getting',\n",
       " 'different',\n",
       " 'requests',\n",
       " 'many',\n",
       " 'appear',\n",
       " 'outside',\n",
       " 'scope',\n",
       " 'position',\n",
       " 'consolidation',\n",
       " 'things',\n",
       " 'left',\n",
       " 'think',\n",
       " 'useful',\n",
       " 'formulate',\n",
       " 'high',\n",
       " 'level',\n",
       " 'project',\n",
       " 'goal',\n",
       " 'make',\n",
       " 'clear',\n",
       " 'possible',\n",
       " 'trying',\n",
       " 'accomplish',\n",
       " 'project',\n",
       " 'help',\n",
       " 'determine',\n",
       " 'requests',\n",
       " 'fall',\n",
       " 'project',\n",
       " 'scope',\n",
       " 'list',\n",
       " 'requests',\n",
       " 'determine',\n",
       " 'scope',\n",
       " 'project',\n",
       " 'fall',\n",
       " 'scope',\n",
       " 'scope',\n",
       " 'work',\n",
       " 'define',\n",
       " 'relative',\n",
       " 'importance',\n",
       " 'priority',\n",
       " 'work',\n",
       " 'traders',\n",
       " 'define',\n",
       " 'exact',\n",
       " 'requirements',\n",
       " 'define',\n",
       " 'desired',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'screen',\n",
       " 'main',\n",
       " 'view',\n",
       " 'drill',\n",
       " 'downs',\n",
       " 'formulate',\n",
       " 'project',\n",
       " 'plan',\n",
       " 'things',\n",
       " 'requested',\n",
       " 'thus',\n",
       " 'particular',\n",
       " 'order',\n",
       " 'inclusion',\n",
       " 'sitara',\n",
       " 'physical',\n",
       " 'deals',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'deal',\n",
       " 'ticker',\n",
       " 'customized',\n",
       " 'rows',\n",
       " 'columns',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'rows columns',\n",
       " 'existing',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'rows columns',\n",
       " 'drill',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'break',\n",
       " 'positions',\n",
       " 'physical',\n",
       " 'transport',\n",
       " 'swaps',\n",
       " 'options',\n",
       " 'addition',\n",
       " 'curve',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'show',\n",
       " 'real time',\n",
       " 'values',\n",
       " 'curves',\n",
       " 'desk',\n",
       " 'position',\n",
       " 'ability',\n",
       " 'split',\n",
       " 'current',\n",
       " 'position',\n",
       " 'grid',\n",
       " 'allow',\n",
       " 'daily',\n",
       " 'positions',\n",
       " 'shown',\n",
       " 'directly',\n",
       " 'monthly',\n",
       " 'positions',\n",
       " 'grouped',\n",
       " 'column',\n",
       " 'grid',\n",
       " 'would',\n",
       " 'tied',\n",
       " 'grouped',\n",
       " 'column',\n",
       " 'bottom',\n",
       " 'grid',\n",
       " 'ability',\n",
       " 'properly',\n",
       " 'show',\n",
       " 'curve',\n",
       " 'shift',\n",
       " 'float for float',\n",
       " 'deals',\n",
       " 'determine',\n",
       " 'appropriate',\n",
       " 'positions',\n",
       " 'show',\n",
       " 'daily',\n",
       " 'monthly',\n",
       " 'index',\n",
       " 'physical',\n",
       " 'nymex',\n",
       " 'physical',\n",
       " 'inside',\n",
       " 'ferc',\n",
       " 'physical',\n",
       " 'market',\n",
       " 'ability',\n",
       " 'pull',\n",
       " 'valuation',\n",
       " 'results',\n",
       " 'based',\n",
       " 'flag',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'official',\n",
       " 'valuations',\n",
       " 'position',\n",
       " 'aggregation',\n",
       " 'across',\n",
       " 'desks',\n",
       " 'ability',\n",
       " 'include',\n",
       " 'price',\n",
       " 'book',\n",
       " 'inclusion',\n",
       " 'spread',\n",
       " 'options',\n",
       " 'systems',\n",
       " 'ability',\n",
       " 'handle',\n",
       " 'volatility',\n",
       " 'skew',\n",
       " 'correlations',\n",
       " 'ability',\n",
       " 'revalue',\n",
       " 'options',\n",
       " 'incrementally',\n",
       " 'throughout',\n",
       " 'trading',\n",
       " 'approximate',\n",
       " 'delta',\n",
       " 'changes',\n",
       " 'valuations',\n",
       " 'using',\n",
       " 'instantaneous',\n",
       " 'gamma',\n",
       " 'gamma',\n",
       " 'grid',\n",
       " 'valuation',\n",
       " 'daily',\n",
       " 'options',\n",
       " 'position',\n",
       " 'screen',\n",
       " 'options',\n",
       " 'months',\n",
       " 'strike',\n",
       " 'delta',\n",
       " 'inclusion',\n",
       " 'positions',\n",
       " 'exotic',\n",
       " 'options',\n",
       " 'currently',\n",
       " 'managed',\n",
       " 'spreadsheets',\n",
       " 'ability',\n",
       " 'isolate',\n",
       " 'position',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'deals',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'ability',\n",
       " 'view',\n",
       " 'change',\n",
       " 'deal',\n",
       " 'deal',\n",
       " 'ticker',\n",
       " 'show',\n",
       " 'deal',\n",
       " 'terms',\n",
       " 'prior',\n",
       " 'deal',\n",
       " 'terms',\n",
       " 'affect',\n",
       " 'change',\n",
       " 'eliminate',\n",
       " 'change',\n",
       " 'deals',\n",
       " 'economic',\n",
       " 'impact',\n",
       " 'deal',\n",
       " 'ticker',\n",
       " 'position',\n",
       " 'drill',\n",
       " 'position',\n",
       " 'manager',\n",
       " 'isolate',\n",
       " 'impact',\n",
       " 'individual',\n",
       " 'deals',\n",
       " 'position',\n",
       " 'total',\n",
       " 'grid',\n",
       " 'cell',\n",
       " 'benchmark',\n",
       " 'positions',\n",
       " 'deployment',\n",
       " 'canada',\n",
       " 'currency',\n",
       " 'volume',\n",
       " 'conversions',\n",
       " 'implicit',\n",
       " 'explicit',\n",
       " 'position',\n",
       " 'break',\n",
       " 'issues',\n",
       " 'allan',\n",
       " 'colleen',\n",
       " 'setting',\n",
       " 'meeting',\n",
       " 'tomorrow',\n",
       " 'discuss',\n",
       " 'direction',\n",
       " 'transport',\n",
       " 'hopefully',\n",
       " 'know',\n",
       " 'much',\n",
       " 'better',\n",
       " 'part',\n",
       " 'stands',\n",
       " 'point']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##another method-test (not used for now)\n",
    "# nltk.download('punkt')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# token_words = word_tokenize(df['body'][0])\n",
    "# filtered_words = [w for w in token_words if not w.lower() in stop_words]\n",
    "# filtered_words = [w.lower() for w in filtered_words]\n",
    "# filtered_words = [w for w in filtered_words if len(w) > 3]\n",
    "# filtered_words = [w for w in filtered_words if not w.isdigit()]\n",
    "# filtered_words = \" \".join([w for w in filtered_words if w not in set(string.punctuation)])\n",
    "# filtered_words = filtered_words.split()\n",
    "# for word in filtered_words:\n",
    "#     if word.isdigit():\n",
    "#         filtered_words.remove(word)\n",
    "#     if not word.isascii():\n",
    "#         filtered_words.remove(word)\n",
    "# filtered_words = [re.sub(r'[^a-zA-Z]', ' ',word) for word in filtered_words]\n",
    "# filtered_words = [w for w in filtered_words if w not in set(string.punctuation)]\n",
    "# filtered_words.split(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
